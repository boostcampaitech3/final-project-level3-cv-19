{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['ghost_net']\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, channel // reduction),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(channel // reduction, channel),        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        y = torch.clamp(y, 0, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def depthwise_conv(inp, oup, kernel_size=3, stride=1, relu=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size, stride, kernel_size//2, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "    )\n",
    "\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n",
    "        super(GhostModule, self).__init__()\n",
    "        self.oup = oup\n",
    "        init_channels = math.ceil(oup / ratio)\n",
    "        new_channels = init_channels*(ratio-1)\n",
    "\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n",
    "            nn.BatchNorm2d(init_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n",
    "            nn.BatchNorm2d(new_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = torch.cat([x1,x2], dim=1)\n",
    "        return out[:,:self.oup,:,:]\n",
    "\n",
    "\n",
    "class GhostBottleneck(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se): \n",
    "        super(GhostBottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            GhostModule(inp, hidden_dim, kernel_size=1, relu=True),\n",
    "            # dw\n",
    "            depthwise_conv(hidden_dim, hidden_dim, kernel_size, stride, relu=False) if stride==2 else nn.Sequential(),\n",
    "            # Squeeze-and-Excite\n",
    "            SELayer(hidden_dim) if use_se else nn.Sequential(),\n",
    "            # pw-linear\n",
    "            GhostModule(hidden_dim, oup, kernel_size=1, relu=False),\n",
    "        )\n",
    "\n",
    "        if stride == 1 and inp == oup:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                depthwise_conv(inp, inp, kernel_size, stride, relu=False),\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)\n",
    "\n",
    "\n",
    "class GhostNet(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=1000, width_mult=1.):\n",
    "        super(GhostNet, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "\n",
    "        # building first layer\n",
    "        output_channel = _make_divisible(16 * width_mult, 4)\n",
    "        layers = [nn.Sequential(\n",
    "            nn.Conv2d(3, output_channel, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )]\n",
    "        input_channel = output_channel\n",
    "        \n",
    "        self.features = []\n",
    "        # building inverted residual blocks\n",
    "        block = GhostBottleneck\n",
    "        for layer in self.cfgs:\n",
    "            for k, exp_size, c, use_se, s in layer: #kernel, exp_size, SEblock, stride\n",
    "                output_channel = _make_divisible(c * width_mult, 4)\n",
    "                hidden_channel = _make_divisible(exp_size * width_mult, 4)\n",
    "                layers.append(block(input_channel, hidden_channel, output_channel, k, s, use_se))\n",
    "                input_channel = output_channel\n",
    "            self.features.append(nn.Sequential(*layers))\n",
    "            layers = []\n",
    "        \n",
    "\n",
    "        # building last several layers\n",
    "        output_channel = _make_divisible(exp_size * width_mult, 4)\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        input_channel = output_channel\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs = {}\n",
    "        stem_out = self.stem(input)\n",
    "        s1 = self.layer1(stem_out) #stage인데 기존 것과 같이쓰기위해 기존 형식을 이용\n",
    "        outputs[\"stem\"] = s1\n",
    "        s2 = self.layer2(s1)\n",
    "        outputs[\"dark2\"] = s2\n",
    "        s3 = self.layer3(s2)\n",
    "        outputs[\"dark3\"] = s3\n",
    "        s4 = self.layer4(s3)\n",
    "        outputs[\"dark4\"] = s4\n",
    "        # s5 = self.layer5(s4)\n",
    "        # outputs[\"dark5\"] = s5\n",
    "        return {k: v for k, v in outputs.items() if k in self.out_features}\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def ghost_net(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a GhostNet model\n",
    "    \"\"\"\n",
    "    \n",
    "    layer1 = [\n",
    "        # k, t, c, SE, s \n",
    "        [3,  16,  16, 0, 1],\n",
    "        [3,  48,  24, 0, 2],\n",
    "        [3,  72,  24, 0, 1],\n",
    "        [5,  72,  40, 1, 2],\n",
    "    ]\n",
    "    layer2 = [\n",
    "        [5, 120,  40, 1, 1],\n",
    "        [3, 240,  80, 0, 2],\n",
    "        [3, 200,  80, 0, 1],\n",
    "        [3, 184,  80, 0, 1],\n",
    "        [3, 184,  80, 0, 1],\n",
    "    ]\n",
    "    layer3 = [\n",
    "        [3, 480, 112, 1, 1],\n",
    "        [3, 672, 112, 1, 1],\n",
    "        [5, 672, 160, 1, 2],\n",
    "        [5, 960, 160, 0, 1],\n",
    "        [5, 960, 160, 1, 1],\n",
    "        [5, 960, 160, 0, 1],\n",
    "        [5, 960, 160, 1, 1]\n",
    "    ]\n",
    "    cfgs = [\n",
    "        layer1, layer2, layer3\n",
    "    ]\n",
    "    return GhostNet(cfgs, **kwargs)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    model = ghost_net()\n",
    "    model.eval()\n",
    "\n",
    "    # print(model)\n",
    "    input = torch.randn(32,3,224,224)\n",
    "\n",
    "    summary(model, input_size=(32,3,224,224),device='cpu')\n",
    "    # y = model(input)\n",
    "    # print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "__all__ = ['ghost_net']\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, channel // reduction),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(channel // reduction, channel),        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        y = torch.clamp(y, 0, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def depthwise_conv(inp, oup, kernel_size=3, stride=1, relu=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size, stride, kernel_size//2, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "    )\n",
    "\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n",
    "        super(GhostModule, self).__init__()\n",
    "        self.oup = oup\n",
    "        init_channels = math.ceil(oup / ratio)\n",
    "        new_channels = init_channels*(ratio-1)\n",
    "\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n",
    "            nn.BatchNorm2d(init_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n",
    "            nn.BatchNorm2d(new_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = torch.cat([x1,x2], dim=1)\n",
    "        return out[:,:self.oup,:,:]\n",
    "\n",
    "\n",
    "class GhostBottleneck(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se): \n",
    "        super(GhostBottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            GhostModule(inp, hidden_dim, kernel_size=1, relu=True),\n",
    "            # dw\n",
    "            depthwise_conv(hidden_dim, hidden_dim, kernel_size, stride, relu=False) if stride==2 else nn.Sequential(),\n",
    "            # Squeeze-and-Excite\n",
    "            SELayer(hidden_dim) if use_se else nn.Sequential(),\n",
    "            # pw-linear\n",
    "            GhostModule(hidden_dim, oup, kernel_size=1, relu=False),\n",
    "        )\n",
    "\n",
    "        if stride == 1 and inp == oup:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                depthwise_conv(inp, inp, kernel_size, stride, relu=False),\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)\n",
    "\n",
    "\n",
    "class GhostNet(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=1000, width_mult=1.):\n",
    "        super(GhostNet, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        self.width_mult = width_mult\n",
    "        # building first layer\n",
    "        self.output_channel = _make_divisible(16 * self.width_mult, 4)\n",
    "        self.layers = [nn.Sequential(\n",
    "            nn.Conv2d(3, self.output_channel, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )]\n",
    "\n",
    "        self.input_channel = self.output_channel\n",
    "        \n",
    "        self.layer1 = self.make_layer(self.cfgs[0])\n",
    "        self.layers = []\n",
    "        self.layer2 = self.make_layer(self.cfgs[1])\n",
    "        self.layers = []\n",
    "        self.layer3 = self.make_layer(self.cfgs[2])   \n",
    "        self.layers = []\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x) \n",
    "        x = self.layer2(x) \n",
    "        x = self.layer3(x) \n",
    "        # x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def make_layer(self, layers_configs):\n",
    "        block = GhostBottleneck\n",
    "        for k, exp_size, c, use_se, s in layers_configs: #kernel, exp_size, SEblock, stride\n",
    "            self.output_channel = _make_divisible(c * self.width_mult, 4)\n",
    "            hidden_channel = _make_divisible(exp_size * self.width_mult, 4)\n",
    "            self.layers.append(block(self.input_channel, hidden_channel, self.output_channel, k, s, use_se))\n",
    "            self.input_channel = self.output_channel\n",
    "        return nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "def ghost_net(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a GhostNet model\n",
    "    \"\"\"\n",
    "    layer1 = [\n",
    "        # k, t, c, SE, s \n",
    "        [3,  16,  16, 0, 1],\n",
    "        [3,  48,  24, 0, 2],\n",
    "        [3,  72,  24, 0, 1],\n",
    "        [5,  72,  40, 1, 2],\n",
    "    ]\n",
    "    layer2 = [\n",
    "        [5, 120,  40, 1, 1],\n",
    "        [3, 240,  80, 0, 2],\n",
    "        [3, 200,  80, 0, 1],\n",
    "        [3, 184,  80, 0, 1],\n",
    "        [3, 184,  80, 0, 1],\n",
    "    ]\n",
    "    layer3 = [\n",
    "        [3, 480, 112, 1, 1],\n",
    "        [3, 672, 112, 1, 1],\n",
    "        [5, 672, 160, 1, 2],\n",
    "        [5, 960, 160, 0, 1],\n",
    "        [5, 960, 160, 1, 1],\n",
    "        [5, 960, 160, 0, 1],\n",
    "        [5, 960, 160, 1, 1]\n",
    "    ]\n",
    "    cfgs = [\n",
    "        layer1, layer2, layer3\n",
    "    ]\n",
    "    return GhostNet(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "if __name__=='__main__':\n",
    "    model = ghost_net()\n",
    "    model.eval()\n",
    "\n",
    "    # print(model)\n",
    "    input = torch.randn(32,3,224,224)\n",
    "\n",
    "    summary(model, input_size=(32,3,224,224),device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "GhostNet                                           --                        --\n",
       "├─Sequential: 1-1                                  [32, 40, 28, 28]          --\n",
       "│    └─Sequential: 2-1                             [32, 16, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-1                            [32, 16, 112, 112]        432\n",
       "│    │    └─BatchNorm2d: 3-2                       [32, 16, 112, 112]        32\n",
       "│    │    └─ReLU: 3-3                              [32, 16, 112, 112]        --\n",
       "│    └─GhostBottleneck: 2-2                        [32, 16, 112, 112]        --\n",
       "│    │    └─Sequential: 3-4                        [32, 16, 112, 112]        464\n",
       "│    │    └─Sequential: 3-5                        [32, 16, 112, 112]        --\n",
       "│    └─GhostBottleneck: 2-3                        [32, 24, 56, 56]          --\n",
       "│    │    └─Sequential: 3-6                        [32, 24, 56, 56]          1,956\n",
       "│    │    └─Sequential: 3-7                        [32, 24, 56, 56]          608\n",
       "│    └─GhostBottleneck: 2-4                        [32, 24, 56, 56]          --\n",
       "│    │    └─Sequential: 3-8                        [32, 24, 56, 56]          2,352\n",
       "│    │    └─Sequential: 3-9                        [32, 24, 56, 56]          --\n",
       "│    └─GhostBottleneck: 2-5                        [32, 40, 28, 28]          --\n",
       "│    │    └─Sequential: 3-10                       [32, 40, 28, 28]          7,658\n",
       "│    │    └─Sequential: 3-11                       [32, 40, 28, 28]          1,688\n",
       "├─Sequential: 1-2                                  [32, 80, 14, 14]          --\n",
       "│    └─GhostBottleneck: 2-6                        [32, 40, 28, 28]          --\n",
       "│    │    └─Sequential: 3-12                       [32, 40, 28, 28]          13,190\n",
       "│    │    └─Sequential: 3-13                       [32, 40, 28, 28]          --\n",
       "│    └─GhostBottleneck: 2-7                        [32, 80, 14, 14]          --\n",
       "│    │    └─Sequential: 3-14                       [32, 80, 14, 14]          19,120\n",
       "│    │    └─Sequential: 3-15                       [32, 80, 14, 14]          3,800\n",
       "│    └─GhostBottleneck: 2-8                        [32, 80, 14, 14]          --\n",
       "│    │    └─Sequential: 3-16                       [32, 80, 14, 14]          17,820\n",
       "│    │    └─Sequential: 3-17                       [32, 80, 14, 14]          --\n",
       "│    └─GhostBottleneck: 2-9                        [32, 80, 14, 14]          --\n",
       "│    │    └─Sequential: 3-18                       [32, 80, 14, 14]          16,436\n",
       "│    │    └─Sequential: 3-19                       [32, 80, 14, 14]          --\n",
       "│    └─GhostBottleneck: 2-10                       [32, 80, 14, 14]          --\n",
       "│    │    └─Sequential: 3-20                       [32, 80, 14, 14]          16,436\n",
       "│    │    └─Sequential: 3-21                       [32, 80, 14, 14]          --\n",
       "├─Sequential: 1-3                                  [32, 160, 7, 7]           --\n",
       "│    └─GhostBottleneck: 2-11                       [32, 112, 14, 14]         --\n",
       "│    │    └─Sequential: 3-22                       [32, 112, 14, 14]         165,728\n",
       "│    │    └─Sequential: 3-23                       [32, 112, 14, 14]         10,064\n",
       "│    └─GhostBottleneck: 2-12                       [32, 112, 14, 14]         --\n",
       "│    │    └─Sequential: 3-24                       [32, 112, 14, 14]         306,992\n",
       "│    │    └─Sequential: 3-25                       [32, 112, 14, 14]         --\n",
       "│    └─GhostBottleneck: 2-13                       [32, 160, 7, 7]           --\n",
       "│    │    └─Sequential: 3-26                       [32, 160, 7, 7]           341,576\n",
       "│    │    └─Sequential: 3-27                       [32, 160, 7, 7]           21,264\n",
       "│    └─GhostBottleneck: 2-14                       [32, 160, 7, 7]           --\n",
       "│    │    └─Sequential: 3-28                       [32, 160, 7, 7]           160,880\n",
       "│    │    └─Sequential: 3-29                       [32, 160, 7, 7]           --\n",
       "│    └─GhostBottleneck: 2-15                       [32, 160, 7, 7]           --\n",
       "│    │    └─Sequential: 3-30                       [32, 160, 7, 7]           622,880\n",
       "│    │    └─Sequential: 3-31                       [32, 160, 7, 7]           --\n",
       "│    └─GhostBottleneck: 2-16                       [32, 160, 7, 7]           --\n",
       "│    │    └─Sequential: 3-32                       [32, 160, 7, 7]           160,880\n",
       "│    │    └─Sequential: 3-33                       [32, 160, 7, 7]           --\n",
       "│    └─GhostBottleneck: 2-17                       [32, 160, 7, 7]           --\n",
       "│    │    └─Sequential: 3-34                       [32, 160, 7, 7]           622,880\n",
       "│    │    └─Sequential: 3-35                       [32, 160, 7, 7]           --\n",
       "====================================================================================================\n",
       "Total params: 2,515,136\n",
       "Trainable params: 2,515,136\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.20\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 1788.73\n",
       "Params size (MB): 10.06\n",
       "Estimated Total Size (MB): 1818.06\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(32,3,224,224),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e5a974c1945f6881837b0cf223050629f773912ff3ef9a6fec0a905654a7e72"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yolox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
